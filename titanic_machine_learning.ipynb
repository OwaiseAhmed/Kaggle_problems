import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import  RandomOverSampler
from sklearn.metrics import accuracy_score

data = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")
test_ids = test["PassengerId"]
data.head()



# cleaning of data is crucial part befr moving on

def clean(data):
  data = data.drop(["Ticket","Cabin","Name","PassengerId"],axis = 1)

  cols = ["SibSp","Parch","Fare","Age"]

  for col in cols:
    # sets the value in the null to the median of the column
    # will give a new dataframe object if inplace is not set to true
    # note that the mean of any column is possible if and only if it is a float or may be an int    

    data[col].fillna(data[col].median(),inplace = True)

  # this is for the Embarked column
  # the idea is to fill the unknown values to "U"
  data.Embarked.fillna("U",inplace = True)

  return data

data  = clean(data)

test = clean(test)


data.head()
test.head()

from sklearn import preprocessing

# changing the important strings to number to be understood
# for that we use preprocessing

le = preprocessing.LabelEncoder()

cols = ["Sex","Embarked"]

# here the change preprocessing is goinf to be observed in the sex an embarked column
for col in cols:
  data[col]  = le.fit_transform(data[col])
  test[col]  = le.fit_transform(test[col])
  # le classes holds the label for each class
  print(le.classes_)

data.head()

# male == 1
# female  == 0


# c =0
# q = 1
# s = 2 
# u =3

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# better than free code camp
y = data["Survived"]
# all the data except survived
X = data.drop("Survived",axis = 1)


# this is really nice
# for ramdom state ==10 it willl change all the time
X_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.2, random_state = 42)

ðŸ’¯
LOGISTIC REGRESSION IS GIVING THE BEST SCORE SO FAR

clf = LogisticRegression()
clf = clf.fit(X_train,y_train)

y_pred = clf.predict(X_val)

submission_preds = clf.predict(test)

df = pd.DataFrame({"PassengerId":test_ids.values,
                    "Survived" : submission_preds,
                   })
df.to_csv("submission_log.csv",index = False)

accuracy_score(y_val, y_pred)

KNN neighbor


from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors = 15)
clf  = clf.fit(X_train, y_train)

y_pred_log = clf.predict(X_val)
accuracy_score(y_val,y_pred_log)





NAIVE BAYES appraoch

from sklearn.naive_bayes import GaussianNB
clf  = GaussianNB()
clf = clf.fit(X_train,y_train)
y_pred_gauss_naive = clf.predict(X_val)
accuracy_score(y_val,y_pred_gauss_naive)

from sklearn.naive_bayes import BernoulliNB
clf = BernoulliNB()
clf = clf.fit(X_train, y_train)
y_pred_multi_naive = clf.predict(X_val)
accuracy_score(y_val,y_pred_gauss_naive)

from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB()
clf = clf.fit(X_train, y_train)
from sklearn.svm import SVC
clf = SVC()
clf = clf.fit(X_train,y_train)

y_pred_svm = clf.predict(X_val)
accuracy_score(y_val,y_pred_svm)
